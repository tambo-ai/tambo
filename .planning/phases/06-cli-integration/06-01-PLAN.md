---
phase: 06-cli-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - cli/src/commands/magic-init.ts
  - cli/src/commands/magic-init.test.ts
autonomous: true
requirements:
  - EXEC-03
  - EXEC-04
  - EXEC-05
  - EXEC-06
  - EXEC-07
  - EXEC-08
  - EXEC-09

must_haves:
  truths:
    - "handleMagicInit orchestrates the full pipeline: analyze → generate plan → confirm → execute"
    - "Analysis phase shows step-by-step spinner updates (framework, components, tools)"
    - "Analysis summary displays counts before confirmation checklist"
    - "Additive re-runs detect existing TamboProvider and registered components, filtering them from the plan"
    - "Errors in analysis prompt user to continue anyway (interactive) or throw (non-interactive)"
    - "Execution phase shows phase-level progress spinners"
    - "Success recap shows files modified, deps installed, and next steps"
    - "Failed items are reported at end with copy-pasteable fix commands"
  artifacts:
    - path: "cli/src/commands/magic-init.ts"
      provides: "handleMagicInit orchestrator, detectExistingSetup, filterPlanForRerun, displayAnalysisSummary"
      exports: ["handleMagicInit"]
    - path: "cli/src/commands/magic-init.test.ts"
      provides: "Tests for magic init orchestrator"
  key_links:
    - from: "cli/src/commands/magic-init.ts"
      to: "cli/src/utils/project-analysis/"
      via: "analyzeProject() call"
    - from: "cli/src/commands/magic-init.ts"
      to: "cli/src/utils/plan-generation/"
      via: "generatePlan() call"
    - from: "cli/src/commands/magic-init.ts"
      to: "cli/src/utils/user-confirmation/"
      via: "confirmPlan() call"
    - from: "cli/src/commands/magic-init.ts"
      to: "cli/src/utils/code-execution/"
      via: "executeCodeChanges() call"
---

<objective>
Build the `handleMagicInit` orchestrator that connects all phase 2-5 modules into a single pipeline with progressive spinner UX, analysis summary, additive re-run detection, and error recovery.

Purpose: This is the core function that makes `--magic` work — it calls analyzeProject, generatePlan, confirmPlan, and executeCodeChanges in sequence with proper UX between each step.

Output: `cli/src/commands/magic-init.ts` with full orchestrator + tests
</objective>

<execution_context>
@/Users/lachlan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/lachlan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-cli-integration/06-CONTEXT.md
@.planning/phases/06-cli-integration/06-RESEARCH.md
@.planning/phases/05-code-execution/05-01-SUMMARY.md
@.planning/phases/05-code-execution/05-02-SUMMARY.md
@cli/src/commands/init.ts
@cli/src/utils/interactive.ts
@cli/src/utils/code-execution/index.ts
@cli/src/utils/project-analysis/
@cli/src/utils/plan-generation/
@cli/src/utils/user-confirmation/
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build handleMagicInit orchestrator with pipeline, UX, and re-run detection</name>
  <files>cli/src/commands/magic-init.ts</files>
  <action>
Create `cli/src/commands/magic-init.ts` with:

**Interface:**

```typescript
interface MagicInitOptions {
  yes?: boolean;
  legacyPeerDeps?: boolean;
  skipAgentDocs?: boolean;
}
```

**`handleMagicInit(options: MagicInitOptions): Promise<void>`** — the main orchestrator:

1. **API key check**: Read API key from `.env.local` via `findTamboApiKey()` or `process.env.TAMBO_API_KEY`. If missing, log "No API key found. Running normal init first..." and call `handleInit()` (import from `./init.js`), then re-check. If still missing, throw Error.

2. **Phase 2 — Analysis** with ora spinner:
   - `spinner.start("Analyzing project...")`
   - Update text through stages: "Detecting framework...", "Detecting components...", "Finding tools..."
   - Call `analyzeProject(process.cwd())` from `../utils/project-analysis/`
   - On success: `spinner.succeed("Analysis complete")`
   - On error: `spinner.fail("Analysis failed")`. If `isInteractive()`, prompt "Could not detect framework. Continue anyway?" via `interactivePrompt`. If not interactive or user says no, throw. If user says yes, use a minimal/empty analysis object to continue.
   - Call `displayAnalysisSummary(analysis)` to show counts

3. **Additive re-run detection**:
   - `detectExistingSetup(analysis)` checks:
     - Read root layout file (from `analysis.structure.rootLayoutPath`), check if content includes "TamboProvider" → `hasProvider: boolean`
     - Check for tambo.ts in lib dir, parse for registered component names → `existingComponents: string[]`
   - If existing setup found, log what's already configured and call `filterPlanForRerun(plan, existing)` to remove already-configured items
   - `filterPlanForRerun` sets `providerSetup` to skipped if hasProvider, filters out components already in existingComponents list

4. **Phase 3 — Plan generation** with spinner:
   - `spinner.start("Generating installation plan...")`
   - Call `generatePlan({ projectAnalysis: analysis, apiKey })` from `../utils/plan-generation/`
   - `spinner.succeed("Plan generated")`
   - On error: `spinner.fail("Plan generation failed")`, throw

5. **Phase 4 — Confirmation**:
   - Call `confirmPlan(plan, { yes: options.yes })` from `../utils/user-confirmation/`
   - If `!confirmation.approved`, log "Setup cancelled" and return

6. **Phase 5 — Execution** with spinner:
   - `spinner.start("Executing changes...")`
   - Update text through: "Installing dependencies...", "Modifying files...", "Verifying setup..."
   - Call `executeCodeChanges(confirmation, { legacyPeerDeps: options.legacyPeerDeps })` from `../utils/code-execution/`
   - On success: `spinner.succeed("Setup complete")`
   - Display recap: files created, files modified, deps installed
   - If `result.errors.length > 0`, show warnings with suggestions
   - Show "Next steps:" with run dev command and docs link
   - On error: `spinner.fail("Execution failed")`, use `categorizeExecutionError` and `formatExecutionError` from code-execution, show fix commands based on error category, then throw

**Helper functions** (not exported, internal to file):

- `displayAnalysisSummary(analysis: ProjectAnalysis)` — logs framework name, component count, tool candidate count, provider count using chalk
- `detectExistingSetup(analysis: ProjectAnalysis)` — returns `{ hasProvider: boolean, existingComponents: string[] }` by reading files
- `filterPlanForRerun(plan: InstallationPlan, existing: {...})` — returns filtered plan with already-set-up items removed
- `getApiKeyForMagic()` — reads from env var or .env.local, returns string | undefined

Use `ora` for spinners (already imported in init.ts pattern). Use `chalk` for colors. Use `isInteractive()` and `interactivePrompt` from `../utils/interactive.js` for prompts. Use `GuidanceError` for non-interactive failures.

Import types from the relevant phase modules. Follow the exact spinner text from CONTEXT.md locked decisions.
</action>
<verify>
`npm run check-types -w cli` passes with no errors. File exists and exports handleMagicInit.
</verify>
<done>handleMagicInit orchestrates the full analyze → plan → confirm → execute pipeline with progressive spinners, analysis summary, additive re-run detection, and error recovery with fix commands.</done>
</task>

<task type="auto">
  <name>Task 2: Add tests for magic init orchestrator</name>
  <files>cli/src/commands/magic-init.test.ts</files>
  <action>
Create `cli/src/commands/magic-init.test.ts` with tests for the orchestrator.

Given Jest ESM limitations (per phase 5 decisions), use integration-style tests with mocked module dependencies rather than trying to mock individual functions.

**Mock all phase modules** at the top level:

- `../utils/project-analysis/` → mock `analyzeProject` returning a fixture ProjectAnalysis
- `../utils/plan-generation/` → mock `generatePlan` returning a fixture InstallationPlan
- `../utils/user-confirmation/` → mock `confirmPlan` returning approved ConfirmationResult
- `../utils/code-execution/` → mock `executeCodeChanges` returning success ExecutionResult
- `../utils/interactive.js` → mock `isInteractive` returning true, `interactivePrompt` as needed
- `ora` → mock returning object with start/stop/succeed/fail/text methods
- `fs` → mock readFileSync/existsSync for API key and re-run detection tests

**Test cases:**

1. "runs full pipeline when API key exists" — verify analyzeProject, generatePlan, confirmPlan, executeCodeChanges called in order
2. "exits gracefully when user cancels confirmation" — confirmPlan returns { approved: false }, verify executeCodeChanges NOT called
3. "shows analysis summary after successful analysis" — verify console.log called with component/tool counts
4. "detects existing setup and filters plan" — mock layout file containing "TamboProvider", verify plan is filtered
5. "handles analysis failure with continue prompt in interactive mode" — analyzeProject throws, mock prompt returning true, verify pipeline continues
6. "throws in non-interactive mode when analysis fails" — isInteractive returns false, analyzeProject throws, expect rejection
7. "passes --yes flag through to confirmPlan" — verify confirmPlan called with { yes: true }

Use `jest.mock()` for module mocking. Keep tests focused on orchestration flow, not on individual module behavior (those are tested in their own test files).
</action>
<verify>
`npm test -- --testPathPatterns='magic-init' -w cli` passes all tests.
</verify>
<done>Magic init orchestrator has test coverage for happy path, cancellation, re-run detection, error handling in interactive and non-interactive modes.</done>
</task>

</tasks>

<verification>
```bash
npm run check-types -w cli
npm run lint -w cli
npm test -- --testPathPatterns='magic-init' -w cli
```

All three pass without errors.
</verification>

<success_criteria>

- handleMagicInit function exists and exports from magic-init.ts
- Full pipeline (analyze → plan → confirm → execute) is orchestrated with spinner UX
- Additive re-run detection filters already-configured items
- Error recovery shows categorized errors with fix commands
- Tests cover happy path, cancellation, re-run, and error scenarios
  </success_criteria>

<output>
After completion, create `.planning/phases/06-cli-integration/06-01-SUMMARY.md`
</output>
