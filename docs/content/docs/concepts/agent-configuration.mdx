---
title: Agent Configuration
description: How Tambo controls agent behavior through Projects, configuration hierarchy, and system instructions
---

Tambo provides a flexible configuration system that controls how AI agents behave in your application. This configuration spans three levels—project, thread, and message—each serving a distinct purpose in shaping agent responses.

## Projects: The Organizational Unit

A **Project** is the top-level organizational unit in Tambo Cloud. Each project contains all the configuration that defines default agent behavior:

- **Custom Instructions** - Default system instructions sent to the LLM with every request
- **LLM Configuration** - Provider selection (OpenAI, Anthropic, Google, etc.) and model choice
- **LLM Parameters** - Model-specific settings like temperature, top_p, top_k stored as JSON
- **Token Limits** - Maximum input tokens (`maxInputTokens`) to cap request size
- **Tool Call Limits** - Maximum tool executions per response (`maxToolCallLimit`)
- **System Prompt Override** - Whether end-users can override project instructions (`allowSystemPromptOverride`)

Projects enable you to maintain separate configurations for different applications or environments. For example, you might have one project for your production chatbot with conservative settings, and another for experimental features with more creative parameters.

## Agent Behavior Components

Agent behavior in Tambo is controlled by four interconnected components:

**Model + Instructions + Tools + Context**

1. **Model** - The LLM provider and model (e.g., GPT-4, Claude Sonnet) configured at the project level
2. **Instructions** - System prompts that guide agent behavior, configured across all three levels
3. **Tools** - Functions the agent can call to take actions or retrieve information
4. **Context** - Additional information provided to the agent at runtime

The configuration system gives you precise control over each of these components at different levels of granularity.

## Configuration Hierarchy

Tambo resolves agent configuration across three levels with an explicit priority order:

### 1. Project-Level (Defaults)

Project configuration provides defaults that apply to all conversations within that project:

- **Custom Instructions** - When a thread is created, project custom instructions are saved as the first system message
- **LLM Settings** - Provider, model, and parameters are resolved from project configuration
- **Limits** - Token and tool call limits apply to all threads

**When to use**: Set project-level configuration for instructions and settings that should apply universally across all conversations. For example: "You are a helpful assistant that provides concise, accurate answers."

### 2. Thread-Level (Conversation-Specific)

Thread configuration can override project instructions for specific conversations:

- **Instruction Override** - If `allowSystemPromptOverride` is enabled in the project, threads can provide a system message in `initialMessages` that completely replaces project custom instructions
- **No Merging** - Thread instructions don't merge with project instructions; they replace them entirely
- **One System Message** - Only one system message is allowed, and it must be the first message

**When to use**: Use thread-level overrides when a specific conversation needs fundamentally different instructions. For example, a debugging session might need: "You are a Python debugging expert. Focus on identifying root causes and suggesting fixes."

### 3. Message-Level (Runtime Context)

Message-level configuration provides dynamic context that augments project and thread instructions:

- **Context Helpers** - The [additional context system](/docs/concepts/additional-context) lets you inject runtime information (current time, user data, file contents, etc.)
- **System Messages** - Message-level system messages add to (rather than replace) project/thread instructions
- **Dynamic Updates** - Context can change with every message without modifying project or thread configuration

**When to use**: Use message-level context for dynamic information that changes per request. For example: current timestamp, user preferences loaded from database, contents of files being edited, search results, etc.

## How Instructions Combine

Understanding how instructions combine across levels is critical for predictable agent behavior:

### Instruction Resolution Order

When a thread is created:

1. **Check for thread override**: If `allowSystemPromptOverride === true` AND `initialMessages` contains a system message → use that as the thread's system message
2. **Else use project instructions**: If `project.customInstructions` exists → create a system message with project instructions
3. **Else no system message**: Thread starts with no default instructions

### Runtime Augmentation

At runtime, when a request is sent to the LLM:

- The thread's system message (from project or thread override) is sent first
- Additional context (via context helpers) is added as supplementary system messages
- These augmentations don't modify the stored thread; they're runtime additions

### Example Flow

```
Project Custom Instructions:
"You are a helpful coding assistant."

Thread Created:
→ System message saved: "You are a helpful coding assistant."

Message Sent with Context Helper:
→ LLM receives:
   1. "You are a helpful coding assistant." (from thread)
   2. "Current file: app.ts\nContents: ..." (from context helper)
   3. User's message
```

## LLM Parameter Customization

Projects support fine-grained control over LLM parameters through the `customLlmParameters` field, which stores provider and model-specific settings as JSON:

### Structure

```json
{
  "openai": {
    "gpt-4o": {
      "temperature": 0.7,
      "top_p": 0.9
    }
  },
  "anthropic": {
    "claude-3-5-sonnet-20241022": {
      "temperature": 0.8
    }
  }
}
```

### Common Parameters

- `temperature` - Controls randomness (0.0 = deterministic, 1.0 = creative)
- `top_p` - Nucleus sampling threshold
- `top_k` - Top-k sampling limit
- `reasoningEffort` - For reasoning models like o1 (low, medium, high)
- Provider-specific parameters (max_tokens, presence_penalty, etc.)

These parameters are resolved when threads are created and persist for that thread's lifetime.

## Custom LLM Endpoints

For the `openai-compatible` provider type, projects can configure custom endpoints:

- `customLlmBaseURL` - Base URL for your custom LLM endpoint
- `customLlmModelName` - Model name to use with the custom endpoint

This enables integration with self-hosted models or alternative providers that implement the OpenAI API format.

## Token and Tool Limits

Projects enforce limits to prevent runaway costs and execution:

### Token Limits

`maxInputTokens` caps the number of input tokens sent to the LLM. When the token limit is reached:

- Tambo truncates context to fit within the limit
- The most recent messages are prioritized
- System instructions are always included

### Tool Call Limits

`maxToolCallLimit` (default: 10) controls how many tool calls the agent can make in a single response. This prevents:

- Infinite loops in tool execution
- Excessive API costs from repeated tool calls
- Runaway agent behavior

## Terminology Mapping

Different parts of the Tambo system use similar terminology that can be confusing. Here's how they map:

| Term | Location | Level | Purpose |
|------|----------|-------|---------|
| **Custom Instructions** | Project settings | Project | Default system message for all threads |
| **System Message Override** | `initialMessages` | Thread | Replaces project instructions for this thread |
| **System Messages** | Additional Context | Message | Runtime context augmentation |

**Key distinction**: Project custom instructions and thread overrides define the core agent persona. Message-level system messages provide dynamic context without changing the core persona.

## Best Practices

### When to Use Each Level

- **Project-level**: Agent persona, tone, core capabilities, default behavior
  - Example: "You are an expert software developer who writes clean, maintainable code"

- **Thread-level**: Conversation-specific specialization
  - Example: "You are helping debug a React application with TypeScript"

- **Message-level**: Dynamic facts, current state, user-specific data
  - Example: Current timestamp, user timezone, contents of files being edited

### Configuration Strategy

1. **Start with project defaults** - Define your agent's core personality and capabilities at the project level
2. **Enable overrides sparingly** - Only set `allowSystemPromptOverride = true` if you need per-conversation customization
3. **Use context helpers for dynamic data** - Don't put user-specific or time-sensitive information in project instructions
4. **Test parameter combinations** - LLM parameters can significantly affect behavior; test thoroughly before deploying

### Common Pitfalls

- **Over-specifying instructions** - Too many instructions can confuse the model. Keep them focused and concise.
- **Mixing static and dynamic** - Don't put timestamps or user IDs in project instructions; use context helpers.
- **Ignoring token limits** - Large custom instructions consume tokens. Balance detail with token efficiency.
- **Forgetting override implications** - When `allowSystemPromptOverride = true`, end-users can completely change agent behavior.

## Related Concepts

- [Additional Context](/docs/concepts/additional-context) - Runtime context injection system
- [Tools](/docs/concepts/tools) - How to register and use agent tools
- [Model Context Protocol](/docs/concepts/model-context-protocol) - Extending agent capabilities with MCP

## Related Guides

- [Configure Project Settings](/docs/guides/configure-agent/project-settings) - Step-by-step project setup
- [Configure LLM Provider](/docs/guides/configure-agent/llm-provider) - Provider and model selection
- [Configure Agent Behavior](/docs/guides/configure-agent/agent-behavior) - Practical configuration patterns
