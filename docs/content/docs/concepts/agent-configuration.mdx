---
title: Agent Configuration
description: Configure your Tambo agent's behavior, LLM provider, MCP servers, and authentication settings
---

Messages sent from your app to Tambo are processed by the Tambo agent, which you can configure through your project settings page. You can add custom instructions to change Tambo's behavior and personality, configure the underlying LLM model and its options, connect MCP servers to extend capabilities, and set up user authentication.

## Custom Instructions

Custom instructions define how your AI agent behaves and responds to users. Think of it as programming the agent's personality, expertise, and communication style.

### What Custom Instructions Do

When you add custom instructions, you're customizing the agent's "system prompt": the foundational instructions that guide every response. This is where you define the agent's role (is it a support agent? a creative partner? a data analyst?), set the tone and personality (formal or casual? friendly or professional?), establish behavioral guidelines for handling different types of requests, and give your AI a name or identity to help users understand who they're talking to.

### Example: Creative Writing Partner

Here's how you might configure custom instructions for a creative writing application:

```
You are a creative writing partner helping authors develop their stories. You're imaginative, collaborative, and supportive. When users share ideas:
- Ask thoughtful questions to deepen their concepts
- Suggest creative alternatives and plot twists
- Help brainstorm character motivations and arcs
- Maintain an encouraging, enthusiastic tone
- Avoid being prescriptive - guide, don't dictate
```

These instructions persist across all conversations in your project, ensuring consistent agent behavior. Users will experience the same personality and approach regardless of which thread they're in.

### Tips for Writing Custom Instructions

Be specific about the agent's expertise. For example, "You specialize in React and TypeScript" is better than "You're a coding assistant." Include behavioral guidelines that tell the agent how to handle edge cases or sensitive situations. Set expectations for response style: should responses be brief? detailed? include examples? And define boundaries around what the agent should refuse to do or redirect.

## LLM Configuration

The LLM (Large Language Model) configuration determines which AI model powers your Tambo agent and how it behaves. You can change the provider, select specific models, and fine-tune parameters to match your application's needs.

### Choosing a Provider and Model

Tambo offers a curated list of models from major providers including OpenAI (GPT-4o, GPT-4o mini, o1, and more), Anthropic (Claude 3.5 Sonnet, Claude 3.7 Opus, Claude 3 Haiku), Google (Gemini 1.5 Pro, Gemini 1.5 Flash, Gemini 2.0 Flash), Groq (fast inference with Llama and other models), and Mistral (Mistral Large, Mistral Small, and specialized models). You can also use any OpenAI-compatible model by selecting the "OpenAI Compatible" provider and specifying a custom endpoint.

### API Keys and Free Messages

Each project starts with free messages to help you get started and test your configuration. Once you've used your free messages, you can add your own API key for the underlying LLM provider. This approach gives you no upfront cost to try Tambo with your project before committing, flexibility to switch to your own API key when you're ready to scale, and cost control by using your own billing and rate limits.

### Custom Parameters

If you need to adjust model-specific behavior, you can configure custom parameters. Temperature controls randomness in responses (0.0 = deterministic, 1.0 = creative), max tokens limits response length, top P controls diversity via nucleus sampling, and top K limits candidate tokens for each generation step. These parameters are stored as JSON and applied when threads are created. Different models support different parameters, so refer to your provider's documentation for available options.

For detailed instructions on configuring LLM providers and parameters, see the [Configure LLM Provider guide](/docs/guides/configure-agent/llm-provider).

## MCP Server Connection

Model Context Protocol (MCP) servers extend your Tambo agent's capabilities by providing access to external tools, data sources, and services. You can connect MCP servers directly from your project settings page, making their capabilities available to all users of your application.

Once connected, your agent can call tools to perform actions and retrieve data, access resources like files, documents, and databases, and use prompts for structured interactions. MCP connections are configured at the project level and shared across all users. This means you set it up once, and every conversation in your project has access to those capabilities.

To learn more about MCP, how it works, and what you can do with it, see the [Model Context Protocol concept page](/docs/concepts/model-context-protocol).

## User Authentication

User authentication determines how Tambo identifies and isolates users in your application. Each user has their own threads and messages, kept separate from other users' data through secure token-based authentication.

You can configure which OAuth provider you're using (Auth0, Clerk, Supabase, etc.), how Tambo verifies user tokens through JWT verification, and how your app's tokens are exchanged for Tambo tokens through the token exchange process. Tambo uses OAuth 2.0 Token Exchange to securely identify users without requiring you to manage separate authentication systems. Your app authenticates users with your provider, then exchanges that token with Tambo for API access.

For complete details on how authentication works and how to integrate with your provider, see the [User Authentication concept page](/docs/concepts/user-authentication).

## Related Guides

Ready to configure your project? The [Configure Project Settings](/docs/guides/configure-agent/project-settings) guide walks you through creating projects and setting up basic configuration. The [Configure LLM Provider](/docs/guides/configure-agent/llm-provider) guide covers choosing models and configuring parameters. And the [Configure Agent Behavior](/docs/guides/configure-agent/agent-behavior) guide provides practical patterns for different use cases.

## Related Concepts

Learn more about injecting runtime context into conversations with [Additional Context](/docs/concepts/additional-context), registering custom tools for your agent to use with [Tools](/docs/concepts/tools), and building dynamic UI components with [Generative Interfaces](/docs/concepts/generative-interfaces).
