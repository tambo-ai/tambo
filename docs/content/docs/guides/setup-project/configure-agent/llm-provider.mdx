---
title: Configure LLM Provider
description: Step-by-step guide to selecting and configuring LLM providers and models
---

This guide walks you through configuring LLM providers and models for your Tambo project, including provider-specific settings and custom parameters.

## Prerequisites

- A configured Tambo Cloud project (see [Configure Project Settings](/docs/guides/setup-project/configure-agent/project-settings))
- API keys for your chosen provider(s)

## Navigating to LLM Settings

1. Open your project in the [Tambo Cloud dashboard](https://cloud.tambo.ai)
2. Navigate to **Settings** or **Project Settings**
3. Find the **LLM Configuration** or **Model Settings** section

## Selecting a Provider

Tambo supports multiple LLM providers. Choose the one that best fits your requirements:

### Available Providers

- **OpenAI** - GPT-4, GPT-4 Turbo, GPT-3.5 models
- **Anthropic** - Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Haiku
- **Google** - Gemini 1.5 Pro, Gemini 1.5 Flash
- **Groq** - Fast inference for Llama, Mixtral models
- **Mistral** - Mistral Large, Mistral Medium
- **OpenAI-Compatible** - Any provider with OpenAI API compatibility

### Selecting Your Provider

1. In the **LLM Configuration** section, find **Provider**
2. Click the dropdown and select your provider
3. The available models list updates based on your selection

## Adding API Keys

Most providers require an API key for authentication.

### Getting API Keys

Visit your provider's dashboard to generate an API key:

- **OpenAI**: [platform.openai.com/api-keys](https://platform.openai.com/api-keys)
- **Anthropic**: [console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys)
- **Google AI Studio**: [makersuite.google.com/app/apikey](https://makersuite.google.com/app/apikey)
- **Groq**: [console.groq.com/keys](https://console.groq.com/keys)
- **Mistral**: [console.mistral.ai/api-keys](https://console.mistral.ai/api-keys)

### Adding the Key to Your Project

1. In **LLM Configuration**, find **API Key** or **Authentication**
2. Click **Add API Key** or paste into the input field
3. Enter your API key exactly as provided by the provider
4. Click **Save** to store the key securely

**Security note:** API keys are encrypted and stored securely. They're never exposed in client-side code or API responses.

## Selecting a Model

After configuring your provider, choose a specific model:

1. In **LLM Configuration**, find **Model** or **Default Model**
2. Click the dropdown to see available models for your provider
3. Select a model based on your needs:

### Model Selection Criteria

- **Capability** - More capable models handle complex tasks better (e.g., GPT-4, Claude 3.5 Sonnet)
- **Cost** - Larger models cost more per token
- **Speed** - Smaller models respond faster (e.g., GPT-3.5, Claude 3 Haiku, Gemini Flash)
- **Context Window** - Some models support larger inputs (e.g., Claude 3.5 Sonnet: 200K tokens)

### Common Model Recommendations

**For production applications:**
- OpenAI: `gpt-4o` or `gpt-4-turbo`
- Anthropic: `claude-3-5-sonnet-20241022`
- Google: `gemini-1.5-pro`

**For cost-sensitive applications:**
- OpenAI: `gpt-3.5-turbo`
- Anthropic: `claude-3-haiku-20240307`
- Google: `gemini-1.5-flash`

**For specialized tasks:**
- Reasoning: `o1-preview`, `o1-mini`
- Vision: `gpt-4-turbo`, `claude-3-5-sonnet-20241022`, `gemini-1.5-pro`
- Coding: `claude-3-5-sonnet-20241022`, `gpt-4o`

For detailed model capabilities, see the [Provider Reference](/docs/reference/providers).

## Configuring Model Parameters

Model parameters control how the LLM generates responses. Tambo stores these as JSON organized by provider and model.

### Accessing Parameter Configuration

1. In **LLM Configuration**, find **Custom Parameters** or **Advanced Settings**
2. Click **Edit Parameters** or **Configure**
3. You'll see a JSON editor or form-based parameter inputs

### Parameter Structure

Parameters are organized hierarchically:

```json
{
  "openai": {
    "gpt-4o": {
      "temperature": 0.7,
      "top_p": 0.9,
      "max_tokens": 2000
    }
  },
  "anthropic": {
    "claude-3-5-sonnet-20241022": {
      "temperature": 0.8,
      "max_tokens": 4000
    }
  }
}
```

### Common Parameters

**`temperature`** (0.0 - 2.0)
- Controls randomness and creativity
- `0.0` - Deterministic, focused responses
- `0.7` - Balanced (recommended default)
- `1.0+` - More creative, varied responses

**`top_p`** (0.0 - 1.0)
- Nucleus sampling threshold
- Controls diversity of word choices
- `0.9` - Recommended default

**`top_k`** (integer)
- Limits sampling to top K tokens
- Lower values = more focused responses
- Not supported by all providers

**`max_tokens`** (integer)
- Maximum length of generated response
- Higher values allow longer responses but cost more

**`presence_penalty`** (-2.0 - 2.0)
- Discourages repetition of any tokens
- Positive values reduce repetition

**`frequency_penalty`** (-2.0 - 2.0)
- Discourages repetition based on frequency
- Positive values reduce repetition

**`reasoningEffort`** (for o1 models)
- Controls reasoning depth: `"low"`, `"medium"`, `"high"`
- Higher effort = more thorough reasoning but slower

### Setting Parameters

**Using JSON editor:**

1. Edit the JSON structure directly
2. Add parameters under the appropriate provider/model path
3. Click **Save**

**Using form inputs:**

1. Select your provider and model
2. Fill in parameter values in the form
3. Click **Save** to apply

### Parameter Examples

**Conservative settings (consistent, focused):**
```json
{
  "openai": {
    "gpt-4o": {
      "temperature": 0.3,
      "top_p": 0.8
    }
  }
}
```

**Balanced settings (default):**
```json
{
  "anthropic": {
    "claude-3-5-sonnet-20241022": {
      "temperature": 0.7,
      "top_p": 0.9
    }
  }
}
```

**Creative settings (varied, exploratory):**
```json
{
  "openai": {
    "gpt-4o": {
      "temperature": 1.2,
      "top_p": 0.95,
      "presence_penalty": 0.5
    }
  }
}
```

## Configuring Custom Endpoints (OpenAI-Compatible)

For self-hosted models or alternative providers, use the **openai-compatible** provider type:

### Setting Up Custom Endpoints

1. Select **openai-compatible** as your provider
2. Locate **Custom Base URL** and **Custom Model Name** fields
3. Enter your endpoint details:

**Custom Base URL:**
- The base URL of your LLM endpoint
- Example: `https://api.together.xyz/v1`
- Must be a valid HTTPS URL

**Custom Model Name:**
- The model identifier your endpoint expects
- Example: `meta-llama/Llama-3-70b-chat-hf`

4. Add your API key if required by the endpoint
5. Click **Save**

### Endpoint Requirements

Your custom endpoint must:
- Implement the OpenAI API format
- Support `/v1/chat/completions` endpoint
- Accept and return OpenAI-compatible message formats
- Handle standard parameters (temperature, max_tokens, etc.)

### Testing Custom Endpoints

1. After configuring, navigate to **Test** or **Playground**
2. Send a test message
3. Verify the response is returned correctly
4. Check logs for any endpoint errors

## Testing Your Configuration

After configuring your provider, model, and parameters:

### Quick Test

1. Navigate to **Test**, **Playground**, or **Try It** in your project
2. Enter a simple test message: `"Hello, can you hear me?"`
3. Click **Send**
4. Verify you receive a response

### Comprehensive Test

Test your configuration with real use cases:

1. **Test custom instructions** - Send a message that should trigger your project's custom instructions
2. **Test parameters** - Try different message types to see if temperature/parameters affect responses appropriately
3. **Test tools** - If you have tools configured, verify the agent can call them
4. **Test context** - If using additional context, verify it's being included

### Load Test (Optional)

For production applications:

1. Send multiple concurrent requests
2. Verify response quality remains consistent
3. Monitor latency and error rates
4. Check provider dashboards for usage patterns

## Troubleshooting

### API Key Errors

**Error: "Invalid API Key"**
- Verify you copied the key correctly (no extra spaces)
- Check the key hasn't been revoked or expired
- Ensure the key belongs to the correct provider account

**Error: "API key required"**
- Add an API key in LLM Configuration
- Verify the key was saved (refresh and check it's still there)

### Model Availability

**Error: "Model not found"**
- Verify the model name matches your provider's model list
- Some models require special access (e.g., GPT-4, o1 models)
- Check your provider account has access to the model

**Error: "Insufficient quota"**
- Add credits to your provider account
- Check usage limits haven't been exceeded
- Verify billing is set up correctly

### Endpoint Validation

**Error: "Invalid endpoint URL"**
- Ensure the URL uses HTTPS (not HTTP)
- Verify the URL is publicly accessible
- Check for typos in the URL

**Error: "Endpoint not responding"**
- Verify your custom endpoint is running
- Check firewall rules allow Tambo's IP addresses
- Test the endpoint directly with curl or Postman

### Parameter Issues

**Responses seem inconsistent**
- High temperature (>1.0) causes more variability
- Reduce temperature for more consistent behavior

**Responses are too short**
- Increase `max_tokens` parameter
- Check token limits in project settings

**Responses are repetitive**
- Increase `presence_penalty` or `frequency_penalty`
- Try a different temperature value

## Best Practices

### Provider Selection

- **Use multiple providers** - Configure failover providers for reliability
- **Match provider to task** - Some models excel at specific tasks (coding, reasoning, creative writing)
- **Monitor costs** - Track usage across providers to optimize costs

### Parameter Tuning

- **Start with defaults** - Begin with temperature=0.7, top_p=0.9
- **Tune incrementally** - Change one parameter at a time
- **Test thoroughly** - Validate changes with real use cases
- **Document settings** - Note why specific parameters were chosen

### Security

- **Rotate keys regularly** - Generate new API keys periodically
- **Use separate keys** - Don't share keys between projects or environments
- **Monitor usage** - Watch for unexpected usage patterns
- **Restrict access** - Limit who can view or modify provider settings

## Next Steps

- [Configure Agent Behavior](/docs/guides/setup-project/configure-agent/agent-behavior) - Practical configuration patterns
- [Provider Reference](/docs/reference/providers) - Detailed provider capabilities
- [Agent Configuration Concepts](/docs/concepts/agent-configuration) - Understanding the system
